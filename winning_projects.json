[
{"title": "opennote", "inspiration": ["<p>STEM is hard. Many students and learners—including us—face the challenge of comprehending STEM notes due to the complex concepts being taught, missed lectures because of schedule conflict, or the pace at which the material is being presented. Limited time and resources of educators only exacerbates this issue.</p>", "<p>Our inspiration stems from our firsthand experience with these challenges, along with the dynamic teaching style of our professor who created animated slides to teach Data Structures and Algorithms. With the majority of students being visual learners, we were motivated to innovate a solution that incorporates digital media with an interactive platform that transforms traditional notes into immersive visual animations.</p>", "<p><strong>opennote</strong> is a multimodal web platform designed to revolutionize the way learners engage with their notes. Whether it’s handwritten or digital, opennote transforms static notes into dynamic animations complete with voiceovers. These animations help students visualize theoretical concepts using models, graphs, and solved examples. </p>", "<p>Each animation is presented as a voiceovered video, accompanied by a chatbot on the right-hand side. This feature enables users to engage with their notes directly, asking questions and gaining clarification on the material.</p>", "<p>opennote supports .png, .pdf, and .jpg files, as well as direct integration with Notion for a seamless user experience. Users have the flexibility to upload additional files to generate more animations, as well as easily share their animations by generating a unique link for each animation. Our website offers the option to clear animation and chatbot history, providing a fresh space for the user at any time.</p>", "<p>We learned…</p>", "<p><a href=\"https://youtu.be/a-5upOy8dPU\" rel=\"nofollow\">https://youtu.be/a-5upOy8dPU</a></p>"], "what_it_does": ["<p><strong>opennote</strong> is a multimodal web platform designed to revolutionize the way learners engage with their notes. Whether it’s handwritten or digital, opennote transforms static notes into dynamic animations complete with voiceovers. These animations help students visualize theoretical concepts using models, graphs, and solved examples. </p>", "<p>Each animation is presented as a voiceovered video, accompanied by a chatbot on the right-hand side. This feature enables users to engage with their notes directly, asking questions and gaining clarification on the material.</p>", "<p>opennote supports .png, .pdf, and .jpg files, as well as direct integration with Notion for a seamless user experience. Users have the flexibility to upload additional files to generate more animations, as well as easily share their animations by generating a unique link for each animation. Our website offers the option to clear animation and chatbot history, providing a fresh space for the user at any time.</p>", "<p>We learned…</p>", "<p><a href=\"https://youtu.be/a-5upOy8dPU\" rel=\"nofollow\">https://youtu.be/a-5upOy8dPU</a></p>"]},
{"title": "MediBuddy", "inspiration": ["<p><strong>MediBuddy is inspired by the grave issue of misdiagnosis in the healthcare world.</strong> </p>", "<p><strong>By leveraging machine learning models, MediBuddy can analyze symptoms, body measurements, and more inputs by doctors and generate accurate, personalized suggestions on diagnoses and treatments based on previous case data.</strong></p>", "<p><strong>Ultimately, the purpose of MediBuddy is not to replace the job of a doctor, but to complement it.</strong></p>", "<p><strong>Roles:</strong></p>", "<p><strong>Frontend:</strong></p>", "<p><strong>Backend:</strong></p>"], "what_it_does": ["<p><strong>Roles:</strong></p>", "<p><strong>Frontend:</strong></p>", "<p><strong>Backend:</strong></p>"]},
{"title": "Clippy", "inspiration": ["<p>We were excited at the prospect of attaching cameras to ourselves and seeing what Gemini would determine as the highlights. </p>", "<p>Over the course of this hackathon, we filmed as much of ourselves having fun as we could—programming, talking to others, playing with a dog named Jackson, trying to remember names, food review, everything—to use as input for Google's Gemini. With the help of Gemini, our project auto-curated a selection of highlights from the many hours of filming complete with selecting transitions, music (generated as well!), and timing.</p>", "<p>The crux of our backend is Gemini Ultra 1.5 (Latest)! Given a very large prompt Sasha wrote, Gemini outputs JSON including timestamps of the cuts of each clip, captions, and transitions to associate with it. A second prompt is then used to compile the clips back together in an order.</p>", "<p>Redis is our database. The music is generated using Udio.</p>", "<p>We developed our frontend with Next.js and React in Typescript. Our component library is mainly based off of ShadCN and has liberal use of animations with Framer motion because we think it looks fun. We used tiktok-uploader to seamlessly add our generated clips to TikTok.</p>", "<p>Our infrastructure fell down and we started integration late when everyone was sleepy. We often found ourselves at odds with the wifi! Sometimes the total length of the video would be hallucinated. </p>", "<p>Thanks to the Google Notebook &amp; Gemini team for letting us bounce ideas and for troubleshooting Gemini with us!</p>"], "what_it_does": ["<p>The crux of our backend is Gemini Ultra 1.5 (Latest)! Given a very large prompt Sasha wrote, Gemini outputs JSON including timestamps of the cuts of each clip, captions, and transitions to associate with it. A second prompt is then used to compile the clips back together in an order.</p>", "<p>Redis is our database. The music is generated using Udio.</p>", "<p>We developed our frontend with Next.js and React in Typescript. Our component library is mainly based off of ShadCN and has liberal use of animations with Framer motion because we think it looks fun. We used tiktok-uploader to seamlessly add our generated clips to TikTok.</p>", "<p>Our infrastructure fell down and we started integration late when everyone was sleepy. We often found ourselves at odds with the wifi! Sometimes the total length of the video would be hallucinated. </p>", "<p>Thanks to the Google Notebook &amp; Gemini team for letting us bounce ideas and for troubleshooting Gemini with us!</p>"]},
{"title": "WhisPath", "inspiration": ["<p>The idea of WhisPath is deeply rooted in addressing the profound challenges faced by visually impaired individuals in navigating their environments. Despite significant advancements in technology, many existing aids still fall short in dynamically informing users about real-time changes in their surroundings. WhisPath is envisioned as a transformative solution that not only enhances traditional navigation aids but also integrates LLM-based artificial intelligence to provide a seamless, interactive navigation experience. This project truly empowers visually impaired people to explore the city area, and helps in day-to-day life. This project is inspired by the potential to significantly enhance the autonomy of visually impaired individuals, allowing them to navigate with confidence and safety through complex environments.</p>", "<p>WhisPath is an innovative application specifically designed to empower visually impaired users by providing auditory guidance about their immediate surroundings. Utilizing the camera on a user’s device, the app captures continuous visual data from the environment. This data is then processed in real-time using a combination of Fetch.ai autonomous agents and Google’s Gemini API. The application performs several critical functions:</p>", "<p>WhisPath introduces a novel integration of LLM-based AI technology with advanced object recognition and natural language processing. This creates a new paradigm for real-time, interactive guidance systems. Currently, we have developed it as a browser-based React-Python application, which takes image input feed continuously on a certain interval and responds with an AI-generated navigation audio guide.  </p>", "<p>The application leverages Fetch.ai agents, each programmed to perform specific tasks: detecting obstacles, assessing threats, and describing the environment. These agents work in an automated fashion, allowing them to process data rapidly and independently, yet synchronize efficiently to provide cohesive feedback. The autonomy of these agents enables the system to adapt to dynamic environments, making real-time decisions without central oversight.</p>", "<p>Google’s Gemini API enhances the capabilities of these agents by providing state-of-the-art image recognition and natural language processing. This API processes the visual data captured by the user’s device camera, identifies relevant objects and hazards, and translates this information into natural language descriptions and warnings.</p>", "<p>The backend of WhisPath, built with Python FastAPI, serves as the communication hub between the Fetch.ai agents and the Gemini API. It processes incoming data from the agents, facilitates API calls, and ensures that the data flow remains smooth and secure. The frontend, developed using React, offers a user-friendly interface that delivers auditory information to the user, allowing for seamless interaction and accessibility.</p>", "<p>This technological synergy between AI-powered agents and powerful API capabilities ensures that WhisPath provides reliable, accurate, and instantaneous navigational aid to visually impaired users.</p>", "<p>Integrating disparate technologies posed significant challenges, particularly in maintaining real-time data processing capabilities and ensuring seamless communication across cross-agents. These challenges were met by adopting a modular architecture, allowing for incremental development and testing. This approach helped the team to isolate and address issues effectively without disrupting the entire system.</p>", "<p>We plan to provide future hardware and mobile software application support with the existing WhisPath codebase for improving user input photo resolution and user experience for the visually impaired. Additionally, we seek to improve the accuracy of our audio guide's traversal suggestion and threat identification and prevention workflows through improved LLM models, parameters, and optimization of agent services.</p>"], "what_it_does": ["<p>WhisPath is an innovative application specifically designed to empower visually impaired users by providing auditory guidance about their immediate surroundings. Utilizing the camera on a user’s device, the app captures continuous visual data from the environment. This data is then processed in real-time using a combination of Fetch.ai autonomous agents and Google’s Gemini API. The application performs several critical functions:</p>", "<p>WhisPath introduces a novel integration of LLM-based AI technology with advanced object recognition and natural language processing. This creates a new paradigm for real-time, interactive guidance systems. Currently, we have developed it as a browser-based React-Python application, which takes image input feed continuously on a certain interval and responds with an AI-generated navigation audio guide.  </p>", "<p>The application leverages Fetch.ai agents, each programmed to perform specific tasks: detecting obstacles, assessing threats, and describing the environment. These agents work in an automated fashion, allowing them to process data rapidly and independently, yet synchronize efficiently to provide cohesive feedback. The autonomy of these agents enables the system to adapt to dynamic environments, making real-time decisions without central oversight.</p>", "<p>Google’s Gemini API enhances the capabilities of these agents by providing state-of-the-art image recognition and natural language processing. This API processes the visual data captured by the user’s device camera, identifies relevant objects and hazards, and translates this information into natural language descriptions and warnings.</p>", "<p>The backend of WhisPath, built with Python FastAPI, serves as the communication hub between the Fetch.ai agents and the Gemini API. It processes incoming data from the agents, facilitates API calls, and ensures that the data flow remains smooth and secure. The frontend, developed using React, offers a user-friendly interface that delivers auditory information to the user, allowing for seamless interaction and accessibility.</p>", "<p>This technological synergy between AI-powered agents and powerful API capabilities ensures that WhisPath provides reliable, accurate, and instantaneous navigational aid to visually impaired users.</p>", "<p>Integrating disparate technologies posed significant challenges, particularly in maintaining real-time data processing capabilities and ensuring seamless communication across cross-agents. These challenges were met by adopting a modular architecture, allowing for incremental development and testing. This approach helped the team to isolate and address issues effectively without disrupting the entire system.</p>", "<p>We plan to provide future hardware and mobile software application support with the existing WhisPath codebase for improving user input photo resolution and user experience for the visually impaired. Additionally, we seek to improve the accuracy of our audio guide's traversal suggestion and threat identification and prevention workflows through improved LLM models, parameters, and optimization of agent services.</p>"]},
{"title": "CallToChange", "inspiration": ["<p>Our inspiration for CallToChange stemmed from the growing need to address carbon emissions generated by technology operations and a way to keep companies accountable and aware of the emissions they produce from AI usage. We aimed to create a tool that empowers companies to measure and mitigate their carbon footprint related to LLM calls.</p>", "<p>CallToChange seamlessly integrates with our custom Python library (call-to-change) to log LLM call emissions. It utilizes academic research and information from the Department of Energy to accurately determine the carbon footprint of each LLM call. Our visually engaging dashboards provide clear insights into emissions data, aiding companies in making informed decisions to reduce their environmental impact.</p>", "<p>We developed CallToChange using Next.js and Python.</p>", "<p>Our Python library logs LLM calls in MongoDB. Under the hood, our library does this by sending a POST request to API routes in our web app. These API routes securely handle data updates in MongoDB.</p>", "<p>In our web app, we use Next.js and MongoDB to fetch user information (such as the number of times a user has used text generation). To create an interactive interface, we utilized Tailwind CSS. To handle user authentication, we used Clerk to manage user accounts.</p>", "<p><img src=\"https://github.com/Arush223/CallToChange/assets/115517528/cabedfe8-b9a7-4473-9759-b6e638612de0\" alt=\"image\"></p>"], "what_it_does": ["<p>CallToChange seamlessly integrates with our custom Python library (call-to-change) to log LLM call emissions. It utilizes academic research and information from the Department of Energy to accurately determine the carbon footprint of each LLM call. Our visually engaging dashboards provide clear insights into emissions data, aiding companies in making informed decisions to reduce their environmental impact.</p>", "<p>We developed CallToChange using Next.js and Python.</p>", "<p>Our Python library logs LLM calls in MongoDB. Under the hood, our library does this by sending a POST request to API routes in our web app. These API routes securely handle data updates in MongoDB.</p>", "<p>In our web app, we use Next.js and MongoDB to fetch user information (such as the number of times a user has used text generation). To create an interactive interface, we utilized Tailwind CSS. To handle user authentication, we used Clerk to manage user accounts.</p>", "<p><img src=\"https://github.com/Arush223/CallToChange/assets/115517528/cabedfe8-b9a7-4473-9759-b6e638612de0\" alt=\"image\"></p>"]},
{"title": "pill.ai", "inspiration": ["<p>Imagine a patient with a chronic illness diligently follows their treatment plan, trusting their pharmacy to provide the correct medications. However, due to a mix-up in prescriptions, they receive the wrong medication, leading to severe complications and a hospitalization. This heartbreaking scenario is all too common in today's healthcare system, where human errors can have devastating consequences.\nBut now, enter <code>pill.ai</code> - the groundbreaking solution designed to prevent such tragedies.</p>", "<p>By harnessing the power of artificial intelligence, <code>pill.ai</code> ensures that each medication is accurately dispensed to the right patient. Our innovative platform takes in multimodal input, processing both image of the medication and audio of the doctor-patient conversation. <code>pill.ai</code> utilizes Google Gemini API which capitalizes the power of Large Language Models (LLMs) to perform speech-to-text and image-to-text transcription. It verifies the correct medication prescribed to the patient based on the audio of doctor-patient conversation, images of the medication received, and references any external information on the intended usage of the medication. </p>", "<p>The patient will be shown a concise dashboard to display all the medication they ought to receive and alerts in real-time if the medication is verified. This significantly reduces the risk of medication-related harm. With <code>pill.ai</code>, patients can have peace of mind knowing that their health is in safe hands.</p>", "<p>We used React, JavaScript, and Material UI to develop the home page while calling Google's Gemini API which employs one million tokens of compute power to perform speech-to-text and image-to-text translation.</p>", "<p>We had difficulties configuring the appropriate JSON format we intend to use for our final dashboard as well as to get speech recognition to work with Google's Gemini API.</p>", "<p>We are proud to have a fully implemented a full-stack web application which uses the latest LLM technology and contribute to a meaningful cause of tackling medication-related errors. </p>", "<p>In our journey with <code>pill.ai</code>, we delved deep into the realm of LLMs and their transformative potential in health tech. LLMs, like GPT-3, enable machines to understand and generate human-like text, empowering <code>pill.ai</code> to analyze vast amounts of medical data efficiently. Additionally, we fully employed the potential of JavaScript to make multiple API calls without the hassle of a backend.</p>", "<p>We hope to make great strides with <code>pill.ai</code> in </p>"], "what_it_does": ["<p>By harnessing the power of artificial intelligence, <code>pill.ai</code> ensures that each medication is accurately dispensed to the right patient. Our innovative platform takes in multimodal input, processing both image of the medication and audio of the doctor-patient conversation. <code>pill.ai</code> utilizes Google Gemini API which capitalizes the power of Large Language Models (LLMs) to perform speech-to-text and image-to-text transcription. It verifies the correct medication prescribed to the patient based on the audio of doctor-patient conversation, images of the medication received, and references any external information on the intended usage of the medication. </p>", "<p>The patient will be shown a concise dashboard to display all the medication they ought to receive and alerts in real-time if the medication is verified. This significantly reduces the risk of medication-related harm. With <code>pill.ai</code>, patients can have peace of mind knowing that their health is in safe hands.</p>", "<p>We used React, JavaScript, and Material UI to develop the home page while calling Google's Gemini API which employs one million tokens of compute power to perform speech-to-text and image-to-text translation.</p>", "<p>We had difficulties configuring the appropriate JSON format we intend to use for our final dashboard as well as to get speech recognition to work with Google's Gemini API.</p>", "<p>We are proud to have a fully implemented a full-stack web application which uses the latest LLM technology and contribute to a meaningful cause of tackling medication-related errors. </p>", "<p>In our journey with <code>pill.ai</code>, we delved deep into the realm of LLMs and their transformative potential in health tech. LLMs, like GPT-3, enable machines to understand and generate human-like text, empowering <code>pill.ai</code> to analyze vast amounts of medical data efficiently. Additionally, we fully employed the potential of JavaScript to make multiple API calls without the hassle of a backend.</p>", "<p>We hope to make great strides with <code>pill.ai</code> in </p>"]},
{"title": "Soundscape", "inspiration": [], "what_it_does": []},
{"title": "GeoCoach", "inspiration": ["<p>Recently, our friends have found a new addiction in Geoguessr. We aren't quite as good as some of them though, but we really want to get better so that we can bet them! To help us improve, we built an AI coach to help us improve!</p>", "<p>GeoCoach is a chrome extension that will expand the functionality of the Geoguessr website seamlessly. Simply start any Geoguessr game and make a guess. Once you've submitted your guess for a round, Tippy, our mascot, will appear and give helpful feedback on your guess. It will provide all kinds of useful information on how to better distinguish between the place you guessed and the correct location. For example, it may note the target location has confierous trees whilst the guess does not or that the guess is significantly less urban or populated than the goal location. It also tracks your stats so you can measure your progress as a player! This information can be seen in chart form or statistical form. It also keeps all the old tips it has given so users can refer back to their previous tips! We also used Gemini to make topological analysis. We are able to analyze what elements of guessing are hardest for a player. </p>", "<p>Our project is a chrome extension that injects scripts into Geoguessr to add tons of new functionality. We started by following Google's chrome extension format and building a simple extension that could inject a script to a website. We then used javascript to build out all of our new UIs by querying the UI and even spying on web requests to dynamically update with the Geoguessr page. We soon realized we would need some sort of backend service to call Gemini for the AI coaching since we shouldn't include the API key in the extension. Plus this allowed us to add a database to track statistics and old hints! We built this backend service in Golang because of the bindings for Gemini.</p>", "<p>We ran into a number of challenges, and many of them seemed to be related to attempting to reverse engineer Geouessr in 36 hours. One major issue was getting everything to load consistently and look correct in the UI. The way we originally hooked into Geoguessr's UI events was ineffective. Sometimes our code would pick up the error but a lot of the time UI wasn't syncing properly. Sometimes our UI would load super slowly and sometimes it wouldn't load at all. We fixed this by creating more custom UI event tracking for each one of our UI features. We also struggled a bit with prompt engineering. Our original prompts gave us inconsistent results and was frankly not good. We worked on the prompt to slim down responses and only give the information that would help us improve.</p>", "<p>We are most proud of how seamless our integration with Geoguessr is. By simply installing an extension in Chrome or Firefox, you unlock a ton of new functionality. Within Geoguessr, you can utilize our app without ever noticing its not part of Geoguessr itself. Our goal was to integrate as smoothly as possible and we we believe that this is was achieved because the UI such as the buttons and dialog are very similar to the original pages' buttons and dialogs.  </p>", "<p>We learned that working with Geo is hard! We also learned that embarking on a reverse engineering heavy project during a hackathon is insane. It took so much of our time just getting our app integrated into the Geoguessr UI. We learned many tricks for reverse engineering on the web such as replacing the fetch method so you can intercept data and use it for triggering events. </p>", "<p>GeoCoach has some room for improvement. We would love to work on building more features to learn how to guess better such as a dedicated practice mode! We can always do more prompt engineering for more accurate and more informative results. </p>", "<p>Google Challenge: <a href=\"https://youtu.be/g9HGAYEFYsQ\" rel=\"nofollow\">Technical Video Demo</a></p>"], "what_it_does": ["<p>GeoCoach is a chrome extension that will expand the functionality of the Geoguessr website seamlessly. Simply start any Geoguessr game and make a guess. Once you've submitted your guess for a round, Tippy, our mascot, will appear and give helpful feedback on your guess. It will provide all kinds of useful information on how to better distinguish between the place you guessed and the correct location. For example, it may note the target location has confierous trees whilst the guess does not or that the guess is significantly less urban or populated than the goal location. It also tracks your stats so you can measure your progress as a player! This information can be seen in chart form or statistical form. It also keeps all the old tips it has given so users can refer back to their previous tips! We also used Gemini to make topological analysis. We are able to analyze what elements of guessing are hardest for a player. </p>", "<p>Our project is a chrome extension that injects scripts into Geoguessr to add tons of new functionality. We started by following Google's chrome extension format and building a simple extension that could inject a script to a website. We then used javascript to build out all of our new UIs by querying the UI and even spying on web requests to dynamically update with the Geoguessr page. We soon realized we would need some sort of backend service to call Gemini for the AI coaching since we shouldn't include the API key in the extension. Plus this allowed us to add a database to track statistics and old hints! We built this backend service in Golang because of the bindings for Gemini.</p>", "<p>We ran into a number of challenges, and many of them seemed to be related to attempting to reverse engineer Geouessr in 36 hours. One major issue was getting everything to load consistently and look correct in the UI. The way we originally hooked into Geoguessr's UI events was ineffective. Sometimes our code would pick up the error but a lot of the time UI wasn't syncing properly. Sometimes our UI would load super slowly and sometimes it wouldn't load at all. We fixed this by creating more custom UI event tracking for each one of our UI features. We also struggled a bit with prompt engineering. Our original prompts gave us inconsistent results and was frankly not good. We worked on the prompt to slim down responses and only give the information that would help us improve.</p>", "<p>We are most proud of how seamless our integration with Geoguessr is. By simply installing an extension in Chrome or Firefox, you unlock a ton of new functionality. Within Geoguessr, you can utilize our app without ever noticing its not part of Geoguessr itself. Our goal was to integrate as smoothly as possible and we we believe that this is was achieved because the UI such as the buttons and dialog are very similar to the original pages' buttons and dialogs.  </p>", "<p>We learned that working with Geo is hard! We also learned that embarking on a reverse engineering heavy project during a hackathon is insane. It took so much of our time just getting our app integrated into the Geoguessr UI. We learned many tricks for reverse engineering on the web such as replacing the fetch method so you can intercept data and use it for triggering events. </p>", "<p>GeoCoach has some room for improvement. We would love to work on building more features to learn how to guess better such as a dedicated practice mode! We can always do more prompt engineering for more accurate and more informative results. </p>", "<p>Google Challenge: <a href=\"https://youtu.be/g9HGAYEFYsQ\" rel=\"nofollow\">Technical Video Demo</a></p>"]},
{"title": "MarkedDown", "inspiration": ["<p>As an avid photographer, I love browsing the web and social media for other people's work, and more often than not I have encountered people presenting some work as their own when it clearly isn't. The issue of plagiarism needs no introduction, as students we have been told the repercussions are severe at the very least. However, for digital media, this is far from true.</p>", "<p>Pirated movies are available everywhere, unreleased songs pop up out of nowhere and it is the actual creators who suffer the most, who are often not credited, or worse, not paid for their work. Stealing a photo is as easy as hitting right-click and download and having stolen digital media taken down is far from easy.</p>", "<p>I got the idea of a low-cost entry level solution last year at Adobe Max where I met a company that engages in the protection of digital media. I was very interested until they told me it's going to cost me 1000$ a month. For hobbyists, that is unreasonably too expensive. Nevertheless, the concept was pretty simple, embed some information in the image which only you know how to reverse and that gives you a proof of identification.</p>", "<p>For LA Hacks, I decided to build on that experience and expand the scope and use more ways to watermark an image to make it more resistant to compression, scaling, adding filters and more.</p>", "<p>MarkedDown is a web application and Chrome extension that help you watermark, identify and trace your digital media online. Since it is bundled as a SaaS platform, it is ready-to-use from signup. The process is as simple as uploading your image, downloading the watermarked copy and using that for all your media. </p>", "<p>The Chrome extension sits in the browser and can passively scan websites for your content intermittently or you can trigger it to scan suspicious websites. In addition, the web app dashboard employs a custom Fetch.ai agent that both routinely and upon request scans specific URLs, returns the images and scans them for your unique watermark.</p>", "<p>With a sleek UI, super response times and easy navigation, MarkedDown hopes to give even the hobby level photographer to be confident about their digital assets.</p>", "<p>I started off by creating the skeleton for all different parts - web app, landing page, extension, node server, python server and Fetch.ai agent. I worked on the APIs first since they were going to be the core functionality of the app. I used Python for the watermarking as it had a reliable library and allowed for customization. I used Node.js for rest of the interaction since it lives in the same SvelteKit project as the web app, reducing load times and makes the overall code easier to comprehend.</p>", "<p>Next, I worked on the web app, where my extensive experience in using SvelteKit for a lot of projects helped me spin it up very quickly. I used daisyUI and TailwindCSS for effortless styling. The daisyUI SaaS landing page template also helped me make my landing page very quickly.</p>", "<p>Alongside the web app, I worked on the Chrome extension. It was surprisingly similar to making a website, which made it pretty straightforward too. Since the beginning, I was also using Google's GenAI model, Gemini 1.5 Pro for image and text embedding. Since the model does not offer image embeddings, I obtained a low temperature description of the image thanks to Gemini's multimodal capabilities which I then embedded as a vector. The vector is used to decide if images found on the web (and then put through the same process) are similar to the photo or not.</p>", "<p>At the very end, as the deadline was approaching, I started to work on integrating Fetch.ai. I used a Fetch.ai Agent to help automate the task of scraping a website and getting the URLs. It was a great feature to add and definitely made coding a tad bit easier. </p>", "<p>I had some difficulties implementing the watermarking approach myself. My initial plan was to create it completely from scratch but I ended up using a library which does something similar (alter data in the image's high frequency domain) and spending more time on indexing the image using other ways.</p>", "<p>Another challenging part was setting up MongoDB as a vector database. I felt the documentation was poorly written and I was not sure of what I did wrong. </p>", "<p>I am very satisfied with how smoothly Kinde authentication worked out. Within 5 mins of starting, I had working authentication in my app with social logins, which I thought was impossible.</p>", "<p>In addition, I am very pleased with the UI/UX of the app. daisyUI's great UI kits and my experience of using it in over 20 web apps finally led me to make what I could consider one of my best design works ever. </p>", "<p>Before LA Hacks, I had no idea how Chrome extensions are built and this was a great learning experience. I did not expect it to be so straightforward, and I was pleasantly surprised. I would surely be making some Chrome extensions in the future.</p>", "<p>I think MarkedDown can do well as an actual SaaS business, as there is a true gap in the market. I hope to integrate more Fetch.ai Agents for automating tasks such as sending takedown requests and adding more media formats such as video and audio.</p>"], "what_it_does": ["<p>MarkedDown is a web application and Chrome extension that help you watermark, identify and trace your digital media online. Since it is bundled as a SaaS platform, it is ready-to-use from signup. The process is as simple as uploading your image, downloading the watermarked copy and using that for all your media. </p>", "<p>The Chrome extension sits in the browser and can passively scan websites for your content intermittently or you can trigger it to scan suspicious websites. In addition, the web app dashboard employs a custom Fetch.ai agent that both routinely and upon request scans specific URLs, returns the images and scans them for your unique watermark.</p>", "<p>With a sleek UI, super response times and easy navigation, MarkedDown hopes to give even the hobby level photographer to be confident about their digital assets.</p>", "<p>I started off by creating the skeleton for all different parts - web app, landing page, extension, node server, python server and Fetch.ai agent. I worked on the APIs first since they were going to be the core functionality of the app. I used Python for the watermarking as it had a reliable library and allowed for customization. I used Node.js for rest of the interaction since it lives in the same SvelteKit project as the web app, reducing load times and makes the overall code easier to comprehend.</p>", "<p>Next, I worked on the web app, where my extensive experience in using SvelteKit for a lot of projects helped me spin it up very quickly. I used daisyUI and TailwindCSS for effortless styling. The daisyUI SaaS landing page template also helped me make my landing page very quickly.</p>", "<p>Alongside the web app, I worked on the Chrome extension. It was surprisingly similar to making a website, which made it pretty straightforward too. Since the beginning, I was also using Google's GenAI model, Gemini 1.5 Pro for image and text embedding. Since the model does not offer image embeddings, I obtained a low temperature description of the image thanks to Gemini's multimodal capabilities which I then embedded as a vector. The vector is used to decide if images found on the web (and then put through the same process) are similar to the photo or not.</p>", "<p>At the very end, as the deadline was approaching, I started to work on integrating Fetch.ai. I used a Fetch.ai Agent to help automate the task of scraping a website and getting the URLs. It was a great feature to add and definitely made coding a tad bit easier. </p>", "<p>I had some difficulties implementing the watermarking approach myself. My initial plan was to create it completely from scratch but I ended up using a library which does something similar (alter data in the image's high frequency domain) and spending more time on indexing the image using other ways.</p>", "<p>Another challenging part was setting up MongoDB as a vector database. I felt the documentation was poorly written and I was not sure of what I did wrong. </p>", "<p>I am very satisfied with how smoothly Kinde authentication worked out. Within 5 mins of starting, I had working authentication in my app with social logins, which I thought was impossible.</p>", "<p>In addition, I am very pleased with the UI/UX of the app. daisyUI's great UI kits and my experience of using it in over 20 web apps finally led me to make what I could consider one of my best design works ever. </p>", "<p>Before LA Hacks, I had no idea how Chrome extensions are built and this was a great learning experience. I did not expect it to be so straightforward, and I was pleasantly surprised. I would surely be making some Chrome extensions in the future.</p>", "<p>I think MarkedDown can do well as an actual SaaS business, as there is a true gap in the market. I hope to integrate more Fetch.ai Agents for automating tasks such as sending takedown requests and adding more media formats such as video and audio.</p>"]},
{"title": "FormFixer", "inspiration": ["<p>Tired of guesswork in the gym? New to lifting and don't know where to start? Say goodbye to uncertainty and hello to precision with FormFixer.</p>", "<p>Imagine stepping into the gym and having a personal trainer at your fingertips, guiding you through every rep, every set, and every exercise with unmatched accuracy. With FormFixer, that vision becomes a reality.</p>", "<p>FormFixer uses state-of-the-art object detection and pose estimation algorithms that seamlessly analyze your every move in near real-time, meticulously evaluating your form and providing actionable insights to optimize your workouts and reduce your risk of injury.</p>", "<p>Join the fitness revolution today with FormFixer, empowering people in their fitness journeys and redefining the way the world trains, one rep at a time.</p>", "<p>FormFixer revolutionizes your fitness journey by offering a seamless platform for analyzing exercise form with precision. By uploading videos of your workouts, such as barbell squats, FormFixer employs advanced pose estimation algorithms and heuristic analysis to meticulously assess crucial metrics like shoulder alignment and squat depth.</p>", "<p>Through data-driven insights, FormFixer delivers personalized feedback tailored to your unique needs, empowering you to make targeted improvements and enhance your performance effectively. With FormFixer, achieving your fitness goals becomes a streamlined and efficient process, guided by actionable feedback derived from cutting-edge technology.</p>", "<p>As we delved into our project, we encountered several hurdles that truly put our collaborative skills to the test. Initially, configuring the Docker container proved to be a formidable challenge, with prolonged installation times and intricate dependency management. Nonetheless, we successfully navigated through the complexities.\nNext, the React Native/Expo tech stack presented its own set of challenges. Since most of us were new to mobile development, we had to learn how to use Expo Go as well as translate our React experience into React Native. Through the learning curve was steep, as mobile dev is quite different from web dev, we were eventually able to adjust to the new environment.<br>\nHowever, the most persistent challenge by far arose when attempting to transmit large video files as multipart data to the backend. Despite our best efforts, achieving seamless file transmission proved to be difficult due to sparse and misleading documentation. Yet, through perseverance and collaborative problem-solving, we were able to use compression and base to overcome this obstacle.</p>", "<p>We were able to build an application that successfully highlights key metrics in one's squatting form. The body alignment is critical for maintaining one's stability and reducing pressure on the spine, and we were able to confirm that… The squat depth metric is important for activating a full range of motion through the exercise. Through our testing, we were able to see that the application showed robustness and responded appropriately to \"good form\" squats and \"bad form\" squats and showed adequate tips for reducing injury and improving performance. Our application has a lot of potential to scale to support more exercises across weightlifting. </p>", "<p>Going into this project, none of us had extensive mobile development experience. As such, we all learned about the React Native/Expo stack and sharpened our mobile development skills. Additionally, we learned a decent amount about YOLO internals as well as other pose estimation techniques. Finally, we learned a whole lot about configuring Docker for development and managing the streaming of large files.</p>", "<p>There are so many features we'd love to add to FormFixer, given the chance. We'd start with adding more support for more exercises, such as deadlift, bench press, yoga, etc. We'd also like to add profiles, allowing you to track your fitness progress and connect with gym friends. We're incredibly proud of what we were able to accomplish at LAHacks 2024, and we hope to be able to continue FormFixer.</p>"], "what_it_does": ["<p>FormFixer revolutionizes your fitness journey by offering a seamless platform for analyzing exercise form with precision. By uploading videos of your workouts, such as barbell squats, FormFixer employs advanced pose estimation algorithms and heuristic analysis to meticulously assess crucial metrics like shoulder alignment and squat depth.</p>", "<p>Through data-driven insights, FormFixer delivers personalized feedback tailored to your unique needs, empowering you to make targeted improvements and enhance your performance effectively. With FormFixer, achieving your fitness goals becomes a streamlined and efficient process, guided by actionable feedback derived from cutting-edge technology.</p>", "<p>As we delved into our project, we encountered several hurdles that truly put our collaborative skills to the test. Initially, configuring the Docker container proved to be a formidable challenge, with prolonged installation times and intricate dependency management. Nonetheless, we successfully navigated through the complexities.\nNext, the React Native/Expo tech stack presented its own set of challenges. Since most of us were new to mobile development, we had to learn how to use Expo Go as well as translate our React experience into React Native. Through the learning curve was steep, as mobile dev is quite different from web dev, we were eventually able to adjust to the new environment.<br>\nHowever, the most persistent challenge by far arose when attempting to transmit large video files as multipart data to the backend. Despite our best efforts, achieving seamless file transmission proved to be difficult due to sparse and misleading documentation. Yet, through perseverance and collaborative problem-solving, we were able to use compression and base to overcome this obstacle.</p>", "<p>We were able to build an application that successfully highlights key metrics in one's squatting form. The body alignment is critical for maintaining one's stability and reducing pressure on the spine, and we were able to confirm that… The squat depth metric is important for activating a full range of motion through the exercise. Through our testing, we were able to see that the application showed robustness and responded appropriately to \"good form\" squats and \"bad form\" squats and showed adequate tips for reducing injury and improving performance. Our application has a lot of potential to scale to support more exercises across weightlifting. </p>", "<p>Going into this project, none of us had extensive mobile development experience. As such, we all learned about the React Native/Expo stack and sharpened our mobile development skills. Additionally, we learned a decent amount about YOLO internals as well as other pose estimation techniques. Finally, we learned a whole lot about configuring Docker for development and managing the streaming of large files.</p>", "<p>There are so many features we'd love to add to FormFixer, given the chance. We'd start with adding more support for more exercises, such as deadlift, bench press, yoga, etc. We'd also like to add profiles, allowing you to track your fitness progress and connect with gym friends. We're incredibly proud of what we were able to accomplish at LAHacks 2024, and we hope to be able to continue FormFixer.</p>"]},
{"title": "Capyble", "inspiration": ["<p>It’s a familiar scenario for many of us: we set out to accomplish one task, only to find ourselves lost in the endless entertainment the internet offers, from one fun app to another. It seems like we’re always playing catch-up, struggling to regain control. That's exactly why we developed Capyble. It started from conversations among friends who were looking for a gentle push to help us stay focused—much like a reminder from our parents to stick to our homework. Capyble acts as that friend who knows just how easily we get distracted and helps steer us back to what’s important, ensuring we make the most of our time and focus on our true priorities.</p>", "<p>Capy acts as your on-screen companion, keeping your schedule and to-do list in mind. Ready to work? Capy engages in a chat with you, asking what's on your agenda and offers suggestions tailored to your habits. While you're working, Capy checks in every 5-10 minutes. If it senses you're off-task—like you're watching YouTube instead of doing your computer science homework that's due tomorrow—it gently reminds you to refocus, thanks to Gemini Vision Pro technology. If Capy's reminder seems out of place—say, you're just choosing some music to work to—you can correct it, and it'll learn from your response. And if you need to revisit an unfinished task, Capy is there to help you find another spot on your calendar shortly.</p>", "<p>Feel free to click and drag Capy around the screen to your heart's content, and you can even fling it off-screen — but watch out, it might just bounce back! This bit of fun is made possible through the use of Flask and React in our design.</p>", "<p>Users can chat with Capy, which assists with daily planning by referring to your calendar and to-do list. This feature is powered by sophisticated prompt-engineered Open AI API and Google Calendar API. </p>", "<p>Do you remember those moments when you wished your mom was right beside you, gently nudging you to focus on your studies? When you are in the 'focus mode', Capy would step in this role. During an active event, Capy initially pauses, then proceeds to capture a screenshot, storing it in a local file. Subsequently, it conducts periodic checks on your activity, employing gemini to verify alignment with the designated event summary.</p>", "<p>Capyble lives in a. WxPython Window we use to render Capy as a sprite, as well as a Electron Desktop App that hosts React components. Information and retrieval is managed by Flask endpoints we built for LLM completions, coordinate passing, event flags, and more, as well as Supabase for scaffolding table-based data such as itemized to-do lists. We leverage Gemini Vision Pro to intelligently evaluate what the user is doing for our focus feature, and other LLMs to build personalities and interact with the user across the span of a greater context.</p>", "<p>Two large challenges presented themselves. The first was, that as we had to run multiple app-style frameworks in parallel, we had to run multiple python scripts. This, alongside a react app that fed to an electron app, meant that sending information was a challenge. While we were ultimately able to wrangle everything together with http requests, it was still an unwieldy project and deployment experience.</p>", "<p>Since our team consisted of hackathon beginners as well, we faced problems with our Github, to the point where we decided to completely migrate the codebase in order to escape an increasing convoluted git tree. Merge conflicts were long and tiresome, as for many of us this was our first time using git in a fast paced setting with others.</p>", "<p>We were all able to expose ourselves to all sorts of technology during this hackathon, and embraced it with vigor. One of our teammates, who \"knew nothing about front-end\" at the start of the hackathon, was building React components and calling endpoints like a pro after the 36 hours. As we learned about the technologies we needed to achieve our vision, we decided to brave the challenges and commit ourselves to rapidly learning in order to productively build.</p>", "<p>We gained a lot of knowledge in working with different types of tools. Additionally, we were able to familiarize ourselves with the collaborative nature of coding projects. We learned about how many unique skills, from UI/UX to deep understanding of frameworks and machine learning were necessary to build a truly complete product. We learned how many hours you can stay up when you care about what you're working on.</p>", "<p>Who doesn't appreciate a charming capybara on their desktop, especially one that's incredibly adorable? Looking ahead, we plan to enhance the capybara's design to increase its cuteness and streamline interactions for a more delightful user experience. After all, it's hard to resist an endearing feature that truly gets you.</p>", "<p>Additionally, we aim to make the backend large language model (LLM) more customizable and secure. We hope to reduce compute costs by caching and vectorizing user data, which additionally helps encrypt it, and play around with using smaller, specialized LLMs for tasks such as image recognition or constraint-based tasks such as schedule planning.</p>"], "what_it_does": []},
{"title": "SafeSignal Blockchain Registry", "inspiration": ["<p>The SafeSignal Blockchain Registry was inspired by the critical need for improved safety and accountability in the use of emergency location beacons like EPIRBs (emergency position indicating radio beacons) and PLBs (personal locator beacons). These devices are essential for rapid response in life-threatening situations, yet there is a gap in standardized, transparent handling and maintenance logs. By integrating blockchain technology with Starknet, we aim to decrease the many problems that may come with the manual processing of beacon registration, such as complexity issues and verification delays.</p>", "<p>Upon the successful purchase of an EPIRB or PLB, a unique smart contract is automatically generated on the Ethereum blockchain. This smart contract serves as a digital representation of the purchased device. Each smart contract contains essential details about the purchased EPIRB or PLB, such as its serial number, owner information, registration status, and any relevant metadata. This information is stored immutably on the blockchain, ensuring transparency and tamper-proof records. The decentralized nature of the blockchain ensures that registration information is transparent and accessible to regulatory authorities and device owners. We also look to reduce the risk of duplicate or invalid registrations and ensure that each device is uniquely and accurately identified on the blockchain. The use of smart contracts streamlines the registration process, reducing administrative overhead and eliminating the need for manual registration procedures. </p>", "<p>Python, Cairo, MongoDB</p>", "<p>We look to expand our project to encompass various types of emergency equipment. We also look to streamline the process for data verification to ensure reliability before initiating smart contracts, in order to maintain data integrity. Lastly, to highlight the importance of device compliance and maintenance, we wish to integrate maintenance features into the smart contracts, such as sending automated safety reminders every two years.</p>"], "what_it_does": ["<p>Upon the successful purchase of an EPIRB or PLB, a unique smart contract is automatically generated on the Ethereum blockchain. This smart contract serves as a digital representation of the purchased device. Each smart contract contains essential details about the purchased EPIRB or PLB, such as its serial number, owner information, registration status, and any relevant metadata. This information is stored immutably on the blockchain, ensuring transparency and tamper-proof records. The decentralized nature of the blockchain ensures that registration information is transparent and accessible to regulatory authorities and device owners. We also look to reduce the risk of duplicate or invalid registrations and ensure that each device is uniquely and accurately identified on the blockchain. The use of smart contracts streamlines the registration process, reducing administrative overhead and eliminating the need for manual registration procedures. </p>", "<p>Python, Cairo, MongoDB</p>", "<p>We look to expand our project to encompass various types of emergency equipment. We also look to streamline the process for data verification to ensure reliability before initiating smart contracts, in order to maintain data integrity. Lastly, to highlight the importance of device compliance and maintenance, we wish to integrate maintenance features into the smart contracts, such as sending automated safety reminders every two years.</p>"]},
{"title": "peculiar grass touching tech", "inspiration": ["<p>The constant hum of the library, the glow of screens late into the night – we, a group of college students ourselves, were all too familiar with the insular nature of college life. Witnessing the toll it took on our well-being and sense of connection, we craved a way to nudge our peers back outside. Inspired by the simple act of stepping onto the grass and feeling the sun on our skin, we envisioned an app that gamifies the outdoors, transforming \"touching grass\" from a joke into a source of stress relief, social connection, and a renewed appreciation for the natural world.</p>", "<p>Our app utilizes your device's camera to detect if you are 1) outside on the grass and 2) touching or petting it. The longer you pet grass, the more points you get!</p>", "<p>We used React Native, a JavaScript-based UI framework for iOS and Android apps. We used TensorFlowJS for the hand pose model and Google Gemini to figure out if there was grass in the image. We also built a demo web version that is based on React.</p>", "<p>The main challenge we ran into was learning how to use react-native and set up our environments. Neither of us have built mobile apps before, so there was a high learning curve in getting started. It was also difficult to get existing TensorFlow models and rendering software to play nicely with react-native.</p>", "<p>In order to test our app, we had to become bona fide \"grass-touchers\" across every inch of the UCLA campus. We swapped the crowded, stale air of Pauley Pavilion for fresh breezes, ditched the phone glow for real sunshine – all in the name of science (and getting some Vitamin D, let's be honest).</p>", "<p>We're on a mission to unleash a global army of grass touchers! We hope that students across the world will be encouraged to go outside, enjoy nature, and compete with their friends for the title of \"Top Turf Titan.\" It's time to ditch the digital and embrace the delightful!</p>"], "what_it_does": ["<p>Our app utilizes your device's camera to detect if you are 1) outside on the grass and 2) touching or petting it. The longer you pet grass, the more points you get!</p>", "<p>We used React Native, a JavaScript-based UI framework for iOS and Android apps. We used TensorFlowJS for the hand pose model and Google Gemini to figure out if there was grass in the image. We also built a demo web version that is based on React.</p>", "<p>The main challenge we ran into was learning how to use react-native and set up our environments. Neither of us have built mobile apps before, so there was a high learning curve in getting started. It was also difficult to get existing TensorFlow models and rendering software to play nicely with react-native.</p>", "<p>In order to test our app, we had to become bona fide \"grass-touchers\" across every inch of the UCLA campus. We swapped the crowded, stale air of Pauley Pavilion for fresh breezes, ditched the phone glow for real sunshine – all in the name of science (and getting some Vitamin D, let's be honest).</p>", "<p>We're on a mission to unleash a global army of grass touchers! We hope that students across the world will be encouraged to go outside, enjoy nature, and compete with their friends for the title of \"Top Turf Titan.\" It's time to ditch the digital and embrace the delightful!</p>"]},
{"title": "GeoSnap", "inspiration": ["<p>People all over the world today gatekeep their favorite spots to visit, like the majestic scenery of parks, the best local restaurants, or perfect areas to take photos. The need for us to go outside to defy the stereotype of typical CS majors, combined with our love for the game of Geoguessr where users attempt to guess the location of a randomly selected Street View location, inspired us to build GeoSnap, a real life version of the game. GeoSnap invites adventurers to step outside, discover hidden gems, and recreate scenic photo challenges, blending real-world exploration with the thrill of competition for digital rewards.  We created GeoSnap to give individuals a chance to go outside and touch some grass in our increasingly digitized world, and explore their community in search of must-see locations around them by getting them to try and recreate other users' photo memories in different scenic locations they choose to upload as Challenges. </p>", "<p>GeoSnap is a web app in which users can upload pictures they took at various locations and submit them as Challenges, at which point other users try to recreate the picture by searching for the real location in their city, physically traveling to the location, then matching up their photo as closely as possible to the original Challenge image. The closer in proximity you are in real life to the original location, and the more your image lines up with the original Challenge image, the higher your score. Users will compete for the highest combined score on a publicly visible leaderboard, with digital prizes for the top ranked users. Our program works by first passing in the user's submitted photo, extracting its real life location longitude and latitude coordinates using .jpg files' embedded geolocation metadata with EXIF. Next, using Google Gemini, our ML model compares the original Challenge image with the competing users' submitted guess images to check for similarity between what objects are common the AI detects in both images, then Gemini outputs a score from 1-5, with 5 being the highest similarity category, for the user's similarity score. To calculate the user's distance score, we utilized the Haversine formula to determine the mile distance between the user's guess image and the original Challenge image, with lower distance figures resulting in a higher score for the user. </p>", "<p>Our backend was built with NodeJS for the Google Gemini and Coordinate Geocoding API implementations, and MongoDB Atlas for the database of images and user identification, while our frontend was developed using TypeScript, React, and NextJS. Our user login system was developed using Auth0.</p>", "<p>We initially ran into issues getting our database set up as well as getting our ML image comparison model to work, however we ironed these problems out and got it working in the end.\nWe ran into several challenges during development, including issues with database implementations causing errors in our UI, figuring out how to link our frontend interface to our backend, routing between the various subpages on our website, as well as with attempting to get our ML image comparison model to function as intended. However, ultimately, we successfully ironed these problems out and were able to deploy a working build of GeoSnap.</p>", "<p>Combined Group Total of 0 hours of sleep and 0 mg of caffeine consumed. On a real note, we are truly proud of our web app's overall usability, fluidity in its UX design, how we've given users the chance to go outside and touch some grass while trying to find people's favorite spots around the world that they try to gatekeep, as well as all the web development skills we learned while overcoming the many difficulties we experienced along the way.</p>", "<p>We learned not only on how to function on effectively 0 hours of sleep, but also about weighing the pros and cons of different APIs, ways of implementing an ML model, and frontend frameworks, as we ended up trying several different options before settling on our current tech stack. We also strengthened our knowledge on building a seamless frontend interface and connecting our user inputs to work well with our web app's backend logic and MongoDB Atlas database.</p>", "<p>To launch GeoSnap on a larger scale, we plan to deploy our web app to a publicly accessible website and mobile app counterpart with a way to take pictures natively within the app. This would streamline and speed up the process of users uploading images to receive their scores as quickly as possible after being compared to the original challenge images.</p>"], "what_it_does": ["<p>GeoSnap is a web app in which users can upload pictures they took at various locations and submit them as Challenges, at which point other users try to recreate the picture by searching for the real location in their city, physically traveling to the location, then matching up their photo as closely as possible to the original Challenge image. The closer in proximity you are in real life to the original location, and the more your image lines up with the original Challenge image, the higher your score. Users will compete for the highest combined score on a publicly visible leaderboard, with digital prizes for the top ranked users. Our program works by first passing in the user's submitted photo, extracting its real life location longitude and latitude coordinates using .jpg files' embedded geolocation metadata with EXIF. Next, using Google Gemini, our ML model compares the original Challenge image with the competing users' submitted guess images to check for similarity between what objects are common the AI detects in both images, then Gemini outputs a score from 1-5, with 5 being the highest similarity category, for the user's similarity score. To calculate the user's distance score, we utilized the Haversine formula to determine the mile distance between the user's guess image and the original Challenge image, with lower distance figures resulting in a higher score for the user. </p>", "<p>Our backend was built with NodeJS for the Google Gemini and Coordinate Geocoding API implementations, and MongoDB Atlas for the database of images and user identification, while our frontend was developed using TypeScript, React, and NextJS. Our user login system was developed using Auth0.</p>", "<p>We initially ran into issues getting our database set up as well as getting our ML image comparison model to work, however we ironed these problems out and got it working in the end.\nWe ran into several challenges during development, including issues with database implementations causing errors in our UI, figuring out how to link our frontend interface to our backend, routing between the various subpages on our website, as well as with attempting to get our ML image comparison model to function as intended. However, ultimately, we successfully ironed these problems out and were able to deploy a working build of GeoSnap.</p>", "<p>Combined Group Total of 0 hours of sleep and 0 mg of caffeine consumed. On a real note, we are truly proud of our web app's overall usability, fluidity in its UX design, how we've given users the chance to go outside and touch some grass while trying to find people's favorite spots around the world that they try to gatekeep, as well as all the web development skills we learned while overcoming the many difficulties we experienced along the way.</p>", "<p>We learned not only on how to function on effectively 0 hours of sleep, but also about weighing the pros and cons of different APIs, ways of implementing an ML model, and frontend frameworks, as we ended up trying several different options before settling on our current tech stack. We also strengthened our knowledge on building a seamless frontend interface and connecting our user inputs to work well with our web app's backend logic and MongoDB Atlas database.</p>", "<p>To launch GeoSnap on a larger scale, we plan to deploy our web app to a publicly accessible website and mobile app counterpart with a way to take pictures natively within the app. This would streamline and speed up the process of users uploading images to receive their scores as quickly as possible after being compared to the original challenge images.</p>"]},
{"title": "PulseWise", "inspiration": ["<p>Our project was inspired by the alarming statistics surrounding heart disease worldwide. With millions of lives at stake each year, especially in the U.S., we were motivated to create a solution that could help individuals better understand and manage their cardiovascular health. </p>", "<p>With early prognosis of the disease, we hope to reduce the complications of patients at risk. Our project is an interactive platform designed to empower users with providing their information and receive feedback on their condition and tailored prevention tips to provide users with a roadmap to wellness. It incorporates an interactive bot for real time assistance, tailored by prevention tips and warnings to provide users with a roadmap to wellness.</p>", "<p>Our project was built using a combination of Reflex (Python), examination of machine learning models for risk prediction, JavaScript, and Tailwind CSS. One notable aspect of our development approach was the integration of the Gemini AI platform to implement the chatbot functionality. This allowed us advance the interactivity and user experience of our solution. We took advantage of the collaboration features on PyCharm to effectively work through the development process and ensure effective teamwork.</p>", "<p>The most significant challenge we encountered was setting up Reflex, as we were not accustomed to coding both the frontend and backend in pure Python. This presented various obstacles, including configuring the correct ports and resolving compatibility issues with our existing codebase. Additionally, processing the machine learning models required meticulous attention to detail, with numerous rounds of trial and error to ensure each factor was accurately accounted for and converted to numerical values. In spite of getting the model to work, Reflex classes made it difficult to connect to one another.</p>", "<p>Despite these challenges, our team is particularly proud of successfully implementing Reflex for the first time in our project. Overcoming this hurdle not only expanded our technical skill set but also demonstrated our team's adaptability and willingness to explore innovative solutions. </p>", "<p>Throughout the development process, we delved into various technologies and experimented with different machine learning models to determine the most accurate approach for our dataset. This hands-on experience provided us with valuable insights into the strengths and limitations of each model, as well as the importance of data preprocessing and feature engineering in improving accuracy. </p>", "<p>Moving forward, we plan to refine and optimize the performance and improve accuracy by experimenting with other machine learning models such as Random Forest. We also plan to expand the capabilities of our chatbot by integrating more advanced conversational AI techniques and improving its ability to provide personalized recommendations. Furthermore, we intend to explore opportunities for scalability and integration with other healthcare platforms to broaden the reach and impact of our solution. </p>"], "what_it_does": ["<p>With early prognosis of the disease, we hope to reduce the complications of patients at risk. Our project is an interactive platform designed to empower users with providing their information and receive feedback on their condition and tailored prevention tips to provide users with a roadmap to wellness. It incorporates an interactive bot for real time assistance, tailored by prevention tips and warnings to provide users with a roadmap to wellness.</p>", "<p>Our project was built using a combination of Reflex (Python), examination of machine learning models for risk prediction, JavaScript, and Tailwind CSS. One notable aspect of our development approach was the integration of the Gemini AI platform to implement the chatbot functionality. This allowed us advance the interactivity and user experience of our solution. We took advantage of the collaboration features on PyCharm to effectively work through the development process and ensure effective teamwork.</p>", "<p>The most significant challenge we encountered was setting up Reflex, as we were not accustomed to coding both the frontend and backend in pure Python. This presented various obstacles, including configuring the correct ports and resolving compatibility issues with our existing codebase. Additionally, processing the machine learning models required meticulous attention to detail, with numerous rounds of trial and error to ensure each factor was accurately accounted for and converted to numerical values. In spite of getting the model to work, Reflex classes made it difficult to connect to one another.</p>", "<p>Despite these challenges, our team is particularly proud of successfully implementing Reflex for the first time in our project. Overcoming this hurdle not only expanded our technical skill set but also demonstrated our team's adaptability and willingness to explore innovative solutions. </p>", "<p>Throughout the development process, we delved into various technologies and experimented with different machine learning models to determine the most accurate approach for our dataset. This hands-on experience provided us with valuable insights into the strengths and limitations of each model, as well as the importance of data preprocessing and feature engineering in improving accuracy. </p>", "<p>Moving forward, we plan to refine and optimize the performance and improve accuracy by experimenting with other machine learning models such as Random Forest. We also plan to expand the capabilities of our chatbot by integrating more advanced conversational AI techniques and improving its ability to provide personalized recommendations. Furthermore, we intend to explore opportunities for scalability and integration with other healthcare platforms to broaden the reach and impact of our solution. </p>"]},
{"title": "PickIt", "inspiration": ["<p>We were inspired by fitness apps that motivate people through points and rewards. We saw an opportunity to apply this concept to environmental cleanup! </p>", "<p>PickIt is a gamified hiking app that encourages eco-friendly behavior. Hikers scan trash they find on trails, earning points based on the environmental impact of collecting the item. They compete with others in their community for the most points and prizes.</p>", "<p>We built PickIt using Reflex, a new framework for us. This presented a learning curve, but we pushed through to create the app.  </p>", "<p>One big challenge was developing a system to score trash based on its ecological impact. We researched different types of waste and their environmental effects to guide a scoring system within the app. </p>", "<p>We're most proud of creating a user-friendly and fun app that promotes positive environmental change. It's exciting to see people using gamification to contribute to a cleaner and healthier planet.</p>", "<p>We learned a new framework (Reflex) and the importance of research in creating an impactful scoring system for different types of trash. </p>", "<p>We're excited to further develop PickIt by making trash collection even more engaging! Imagine scanning trash and seeing it transform into a fun cartoon character. We're also exploring ways to make collected trash tradable between users. </p>"], "what_it_does": ["<p>PickIt is a gamified hiking app that encourages eco-friendly behavior. Hikers scan trash they find on trails, earning points based on the environmental impact of collecting the item. They compete with others in their community for the most points and prizes.</p>", "<p>We built PickIt using Reflex, a new framework for us. This presented a learning curve, but we pushed through to create the app.  </p>", "<p>One big challenge was developing a system to score trash based on its ecological impact. We researched different types of waste and their environmental effects to guide a scoring system within the app. </p>", "<p>We're most proud of creating a user-friendly and fun app that promotes positive environmental change. It's exciting to see people using gamification to contribute to a cleaner and healthier planet.</p>", "<p>We learned a new framework (Reflex) and the importance of research in creating an impactful scoring system for different types of trash. </p>", "<p>We're excited to further develop PickIt by making trash collection even more engaging! Imagine scanning trash and seeing it transform into a fun cartoon character. We're also exploring ways to make collected trash tradable between users. </p>"]},
{"title": "Aide-n", "inspiration": ["<p>In emergency situations, quick access to accurate information can be critical. A web app that accepts both text and image inputs allows for intuitive interaction, particularly in stressful situations where typing detailed descriptions might be challenging. This could be especially beneficial for users who are visually impaired, have limited mobility, or are not native speakers of the language. This innovative approach leverages technology to make medical information more dynamic, accessible, and practical, bridging the gap between laypersons and the often complex field of medical care.</p>", "<p>Aide-n is a platform that allows users to input an image of an injury which will then be taken and transformed into a text description. The description is presented as an output, and also taken as an input into an Intel Large Language Model. The LLM uses text to text modeling to generate a potential solution to the injury that was described. </p>", "<p>The model was built using two parts. The first part invovled creating an image to text generation utilizing Google's Gemini API. The second part involved taking the textual description of the image to generate a potential solution to the injury. To do this we built an LLM using Intel's architecture including their Intel Developer Cloud as well as pytorch-gpu.</p>", "<p>We ran into many challenges. The first challenge was figuring out how to utilize Intel's architecture including their jupyter notebooks to run our code. Since our code was a LLM, it had to take a significant amount of power which required us to run on Intel's developer cloud rather than our local machines that we could integrate with our UI/UX. This lead to many issues afterwards including the integration of Gemini and our LLMs. We had also incorporated our own instances of Small VM - Intel Xeon 4th Gen Scalable processor which were extremely hard to navigate due to their remote nature.</p>", "<p>We are proud of being able to learn so much about creating and executing Large Language Models. In addition the idea that we could connect onto a remote processor that had a significnat more power than our local machines was really cool! We're also proud of being able to pull all-nighters :)</p>", "<p>We learned about LLMs, Gemini, remote processors, and so many companies as well, all in a really short time.</p>", "<p>Aide-n is essentially encompassing two models, that are trained on datasets that we can change at our will. This means that Aide-n is not just limited to First Aid but rather can be trainined on medical images,  research papers, and so much more. The potential use cases of these models has so many possibilities and it's exciting to think about them!</p>"], "what_it_does": ["<p>Aide-n is a platform that allows users to input an image of an injury which will then be taken and transformed into a text description. The description is presented as an output, and also taken as an input into an Intel Large Language Model. The LLM uses text to text modeling to generate a potential solution to the injury that was described. </p>", "<p>The model was built using two parts. The first part invovled creating an image to text generation utilizing Google's Gemini API. The second part involved taking the textual description of the image to generate a potential solution to the injury. To do this we built an LLM using Intel's architecture including their Intel Developer Cloud as well as pytorch-gpu.</p>", "<p>We ran into many challenges. The first challenge was figuring out how to utilize Intel's architecture including their jupyter notebooks to run our code. Since our code was a LLM, it had to take a significant amount of power which required us to run on Intel's developer cloud rather than our local machines that we could integrate with our UI/UX. This lead to many issues afterwards including the integration of Gemini and our LLMs. We had also incorporated our own instances of Small VM - Intel Xeon 4th Gen Scalable processor which were extremely hard to navigate due to their remote nature.</p>", "<p>We are proud of being able to learn so much about creating and executing Large Language Models. In addition the idea that we could connect onto a remote processor that had a significnat more power than our local machines was really cool! We're also proud of being able to pull all-nighters :)</p>", "<p>We learned about LLMs, Gemini, remote processors, and so many companies as well, all in a really short time.</p>", "<p>Aide-n is essentially encompassing two models, that are trained on datasets that we can change at our will. This means that Aide-n is not just limited to First Aid but rather can be trainined on medical images,  research papers, and so much more. The potential use cases of these models has so many possibilities and it's exciting to think about them!</p>"]},
{"title": "EcoEval", "inspiration": ["<p>Driven by our collective aspiration to promote a greener planet through mindful consumerism, we conceptualized EcoEval. We recognized a gap in the market for a comprehensive, user-friendly tool that could provide online shoppers with detailed sustainability profiles of the products they’re considering. While existing solutions like ‘CarbonCheck - Discover Sustainable Products’ offer valuable insights into a product’s carbon footprint, we felt they only told half the story. We wanted to consider other factors such as the materials used and the manufacturing process. EcoEval was born out of this desire to provide a more holistic view of a product’s environmental impact, turning every online shopping experience into an opportunity for positive change.</p>", "<p>EcoEval, our Chrome extension, presents users with a detailed sustainability profile of the product they’re currently viewing. This profile includes a sustainability sentiment, a breakdown of the product’s sustainable attributes (like the materials used), an overview of the manufacturer’s sustainability ethos, and suggestions for alternative sustainable products.</p>", "<p>We start by extracting product information, including its name and manufacturer, from the URL of the user’s current tab. We then leverage Google’s Gemini to generate a sustainability rating, a summary of the product’s sustainable aspects, a description of the manufacturer’s sustainability values, and the product type. Using the product type, we perform web scraping with BeautifulSoup on langchain-community to find related sustainable products. All this information is then sent to our frontend, which is built with React, to be displayed to the user.</p>", "<p>One of the main challenges we faced was in the realm of frontend development, specifically with the creation of our Chrome extension. Our choice to develop a Chrome extension was driven by our desire to provide users with a seamless shopping experience. We wanted to avoid the inconvenience of having to navigate away from their shopping page to a separate webpage. With our extension, sustainability information is just one click away and allows for live comparison as users browse products.</p>", "<p>However, developing a Chrome extension was a new experience for our team. We had to quickly familiarize ourselves with the unique aspects of Chrome extension development. This included understanding various Chrome extension features such as popups, side panels, and tab detection. We also had to learn how the side panel communicates with the webpage, and how to control the frontend of the side panel and the main page separately.</p>", "<p>Another challenge was ensuring that our extension would work seamlessly across different websites, given the variability in website structures. We had to ensure that our extension could accurately extract product information from various websites, which required us to develop robust and flexible code.</p>", "<p>Despite these challenges, we are proud of what we have accomplished. We believe that the decision to create a Chrome extension significantly enhances the user experience of EcoEval, making sustainable shopping more accessible and convenient for all. We look forward to continuing to improve and expand upon what we have built.</p>", "<p>We are proud that we were able to use new technologies (i.e. Gemini) and the knowledge we learned from different workshops about Gen AI, and use it to generate useful information (i.e. summary of how sustainable the product materials are) about sustainability. Because it is also our first time developing a chrome extension, we are proud that we were able to learn quickly about different features chrome extensions offer, how interactions between the webpage and the side panel works, and be able to adjust our frontend design as we explore more suitable features for our project.</p>", "<p>We learned to use different technologies through various sponsored workshops. Considering the technologies we learned from the workshops (i..e. Google Gemini) and the existing technologies that we already know how to use (i.e. React, Beautiful Soup), we decided which ones are more suitable for our project. We also researched and learned to use new technologies (i.e. oxylabs scrapper APIs). </p>", "<p>While our current focus is on providing sustainability reports for products sold on Amazon, we plan to extend our support to other platforms. We also aim to enhance the accuracy of our system by incorporating the latest news articles about a company’s sustainability practices. We’ve found that while many companies claim to have ethical and eco-friendly practices, it’s often through media criticism that their failures to act on these claims are exposed. By integrating the Gnews API and further developing our Gemini prompt, we hope to provide more accurate information. We’re also considering a points and rewards system based on how much a user spends on eco-friendly products, which would depend on partnerships with eco-friendly companies. Currently, we provide three sustainability responses (yes, maybe, or no), but we hope to create a database for more statistical evaluations of a company based on different parameters. With this database, we aim to train a model to provide a score-based response rather than a sentiment analysis-based response.</p>"], "what_it_does": ["<p>EcoEval, our Chrome extension, presents users with a detailed sustainability profile of the product they’re currently viewing. This profile includes a sustainability sentiment, a breakdown of the product’s sustainable attributes (like the materials used), an overview of the manufacturer’s sustainability ethos, and suggestions for alternative sustainable products.</p>", "<p>We start by extracting product information, including its name and manufacturer, from the URL of the user’s current tab. We then leverage Google’s Gemini to generate a sustainability rating, a summary of the product’s sustainable aspects, a description of the manufacturer’s sustainability values, and the product type. Using the product type, we perform web scraping with BeautifulSoup on langchain-community to find related sustainable products. All this information is then sent to our frontend, which is built with React, to be displayed to the user.</p>", "<p>One of the main challenges we faced was in the realm of frontend development, specifically with the creation of our Chrome extension. Our choice to develop a Chrome extension was driven by our desire to provide users with a seamless shopping experience. We wanted to avoid the inconvenience of having to navigate away from their shopping page to a separate webpage. With our extension, sustainability information is just one click away and allows for live comparison as users browse products.</p>", "<p>However, developing a Chrome extension was a new experience for our team. We had to quickly familiarize ourselves with the unique aspects of Chrome extension development. This included understanding various Chrome extension features such as popups, side panels, and tab detection. We also had to learn how the side panel communicates with the webpage, and how to control the frontend of the side panel and the main page separately.</p>", "<p>Another challenge was ensuring that our extension would work seamlessly across different websites, given the variability in website structures. We had to ensure that our extension could accurately extract product information from various websites, which required us to develop robust and flexible code.</p>", "<p>Despite these challenges, we are proud of what we have accomplished. We believe that the decision to create a Chrome extension significantly enhances the user experience of EcoEval, making sustainable shopping more accessible and convenient for all. We look forward to continuing to improve and expand upon what we have built.</p>", "<p>We are proud that we were able to use new technologies (i.e. Gemini) and the knowledge we learned from different workshops about Gen AI, and use it to generate useful information (i.e. summary of how sustainable the product materials are) about sustainability. Because it is also our first time developing a chrome extension, we are proud that we were able to learn quickly about different features chrome extensions offer, how interactions between the webpage and the side panel works, and be able to adjust our frontend design as we explore more suitable features for our project.</p>", "<p>We learned to use different technologies through various sponsored workshops. Considering the technologies we learned from the workshops (i..e. Google Gemini) and the existing technologies that we already know how to use (i.e. React, Beautiful Soup), we decided which ones are more suitable for our project. We also researched and learned to use new technologies (i.e. oxylabs scrapper APIs). </p>", "<p>While our current focus is on providing sustainability reports for products sold on Amazon, we plan to extend our support to other platforms. We also aim to enhance the accuracy of our system by incorporating the latest news articles about a company’s sustainability practices. We’ve found that while many companies claim to have ethical and eco-friendly practices, it’s often through media criticism that their failures to act on these claims are exposed. By integrating the Gnews API and further developing our Gemini prompt, we hope to provide more accurate information. We’re also considering a points and rewards system based on how much a user spends on eco-friendly products, which would depend on partnerships with eco-friendly companies. Currently, we provide three sustainability responses (yes, maybe, or no), but we hope to create a database for more statistical evaluations of a company based on different parameters. With this database, we aim to train a model to provide a score-based response rather than a sentiment analysis-based response.</p>"]},
{"title": "Scribe", "inspiration": [], "what_it_does": []},
{"title": "HarmonicHomes", "inspiration": ["<p>Smart homes are not a seamless experience. All smart home enthusiast love the idea of automating things around the house and being able to easily control their home with their voice but, we often get disappointed. Google Assistant, Siri, and Alexa all look for specific phrasing, and if we don't say the right thing it may not work at all. On top of that, we need to download many apps to manage all our smart devices. Rather than use dashboards we should be able to use our voice. And with the power of AI our smart home should understand us and just work. Lastly, wouldn't it be great to have a smart home handle more complex interactions for us by using the devices in our homes? What if there's an intruder? I may imagine the smartest home locking the door, playing sirens, and turning the lights to police colors. No smart home can do that yet but, what if it could?</p>", "<p>Harmonic Homes leverages AI agents in a tool former (<a href=\"https://arxiv.org/abs/2302.04761\" rel=\"nofollow\">https://arxiv.org/abs/2302.04761</a>) configuration to interpret natural language and control the simulated smart home devices. Harmonic Homes identifies if a user's request can be handled by an existing device command and executes the appropriate ones. If the user requests a more complex interaction the smart home will identify a relevant function it has created or it will create an entirely new function. Essentially, Harmonic Homes aims to make smarthome systems even smarter.</p>", "<p>To build the user-facing application, we created a NextJS application. We've designed the UI using Tailwind and took components from Aceternity to make user interfaces that are both intuitive and easy to use. Then, the application workflow works as follows: users would input a query/command that would be sent to the back</p>", "<p>Ultimately, the heart of HarmonicHomes lies in the backend. First of all, our backend consists of three agents created using the uAgents framework: the orchestrator, the tool-former, and a tool-verifier.</p>", "<p>The Orchestrator agent handles simple function-calling for functionalities that already exist in one's smart home. We've defined various general functionalities, such as \"change_room_light\", that the orchestrator both understands and recognizes when given a prompt.\nWe utilized the Gemini API, specifically the Gemini Pro model, due to its function-calling functionality. We can respond to user responses promptly, and pass in specific desired parameters to the functions that already exist. This way, simple commands can be carried out instantaneously</p>", "<p>That being said, there are a lot of potential prompts and commands that users may want to pass into their smart home; for example, a user may tell their home devices \"An intruder arrives!\". In that case, our homes should identify a complex behavior, such as turning sirens onto the speakers across various rooms, change the lights to emergency-based colors, etc... In another example, a user might want their house to be Christmas-themed: this means the lights in the rooms would be red, green, and white. The speakers, then, would play Christmas music.\nThese are complex behaviors that the orchestrator will NOT know beforehand; thus, it'll send the user's complex command to the tool-former. The tool-former will know all of the simple, pre-defined smart-home actions, and build new functions/behaviors that combine the pre-defined functions. For example, a new function \"handle_intruder\" would combine simple actions (turning the sirens on, setting emergency colors). \nThis newly created function, then, would get added to the list of smart-home functions. This way, users can REUSE their previously created complex behaviors, such that the tool-former doesn't need to \"remake\" these functions.</p>", "<p>Code doesn't always work the way we want it to. This especially applies to code generated through the Gemini API; we need a way to confirm that the function works before we add the complex behavior(s) to the list of \"known functions\". Otherwise, the orchestrator would attempt to run a complex function that doesn't work.\nTo solve this issue, we created a third agent: the tool-verifier. In short terms, we've automated quality assurance. Think of it as one AI agent grading the output of another AI agent. When the tool-former creates a new function, we begin a feedback loop between the tool-former and the tool-verifier. The tool-verifier is equipped with documentation on how functions in the smart home work. Thus, the feedback loop will persist in communication, forcing the tool-former to remake the new functions until the tool-verifier is satisfied with the output.</p>", "<p>The main challenge we ran into was rate-limiting. We really loved Gemini's function-calling capabilities and wanted to use it to carry out all of our smart-home functions (ex. turn on all of the lights, change all the lights to red). \nHowever, we ran into issues because each smart-home interaction was a call to the Gemini API; thus, complex behaviors, consisting of multiple smart-home interactions, would be rate-limited (429 errors).  The error, however, logged to our agents was \"index out of bounds\"; this didn't make sense to us.</p>", "<p>Additionally, many concepts relating to agents were previously foreign to us; we had to overcome an initial mental block of understanding how to integrate agents into our workflow and how exactly we could implement the ideas described in the research paper.</p>", "<p>We're really proud of the tool-former functionality; we've made agents that created new functions that the smart-home recognizes AND can reuse. We're also super proud of creating a working feedback agent - one that could essentially verify the correctness of the code generated by the fool former agent, and have it regenerate code until it gets it right.</p>", "<p>We learned a lot about prompt engineering like few-shot learning, role, and effective use of context windows.</p>", "<p>Due to the fast pace of the hackathon setting, our team was forced to push off some features in order to complete the main functionality as fast as possible. One feature we wish we could have implemented is automatic documentation generation for newly created smart home commands, allowing our gemini model to instantly expand its context and capabilities according to the user's commands.</p>"], "what_it_does": ["<p>Harmonic Homes leverages AI agents in a tool former (<a href=\"https://arxiv.org/abs/2302.04761\" rel=\"nofollow\">https://arxiv.org/abs/2302.04761</a>) configuration to interpret natural language and control the simulated smart home devices. Harmonic Homes identifies if a user's request can be handled by an existing device command and executes the appropriate ones. If the user requests a more complex interaction the smart home will identify a relevant function it has created or it will create an entirely new function. Essentially, Harmonic Homes aims to make smarthome systems even smarter.</p>", "<p>To build the user-facing application, we created a NextJS application. We've designed the UI using Tailwind and took components from Aceternity to make user interfaces that are both intuitive and easy to use. Then, the application workflow works as follows: users would input a query/command that would be sent to the back</p>", "<p>Ultimately, the heart of HarmonicHomes lies in the backend. First of all, our backend consists of three agents created using the uAgents framework: the orchestrator, the tool-former, and a tool-verifier.</p>", "<p>The Orchestrator agent handles simple function-calling for functionalities that already exist in one's smart home. We've defined various general functionalities, such as \"change_room_light\", that the orchestrator both understands and recognizes when given a prompt.\nWe utilized the Gemini API, specifically the Gemini Pro model, due to its function-calling functionality. We can respond to user responses promptly, and pass in specific desired parameters to the functions that already exist. This way, simple commands can be carried out instantaneously</p>", "<p>That being said, there are a lot of potential prompts and commands that users may want to pass into their smart home; for example, a user may tell their home devices \"An intruder arrives!\". In that case, our homes should identify a complex behavior, such as turning sirens onto the speakers across various rooms, change the lights to emergency-based colors, etc... In another example, a user might want their house to be Christmas-themed: this means the lights in the rooms would be red, green, and white. The speakers, then, would play Christmas music.\nThese are complex behaviors that the orchestrator will NOT know beforehand; thus, it'll send the user's complex command to the tool-former. The tool-former will know all of the simple, pre-defined smart-home actions, and build new functions/behaviors that combine the pre-defined functions. For example, a new function \"handle_intruder\" would combine simple actions (turning the sirens on, setting emergency colors). \nThis newly created function, then, would get added to the list of smart-home functions. This way, users can REUSE their previously created complex behaviors, such that the tool-former doesn't need to \"remake\" these functions.</p>", "<p>Code doesn't always work the way we want it to. This especially applies to code generated through the Gemini API; we need a way to confirm that the function works before we add the complex behavior(s) to the list of \"known functions\". Otherwise, the orchestrator would attempt to run a complex function that doesn't work.\nTo solve this issue, we created a third agent: the tool-verifier. In short terms, we've automated quality assurance. Think of it as one AI agent grading the output of another AI agent. When the tool-former creates a new function, we begin a feedback loop between the tool-former and the tool-verifier. The tool-verifier is equipped with documentation on how functions in the smart home work. Thus, the feedback loop will persist in communication, forcing the tool-former to remake the new functions until the tool-verifier is satisfied with the output.</p>", "<p>The main challenge we ran into was rate-limiting. We really loved Gemini's function-calling capabilities and wanted to use it to carry out all of our smart-home functions (ex. turn on all of the lights, change all the lights to red). \nHowever, we ran into issues because each smart-home interaction was a call to the Gemini API; thus, complex behaviors, consisting of multiple smart-home interactions, would be rate-limited (429 errors).  The error, however, logged to our agents was \"index out of bounds\"; this didn't make sense to us.</p>", "<p>Additionally, many concepts relating to agents were previously foreign to us; we had to overcome an initial mental block of understanding how to integrate agents into our workflow and how exactly we could implement the ideas described in the research paper.</p>", "<p>We're really proud of the tool-former functionality; we've made agents that created new functions that the smart-home recognizes AND can reuse. We're also super proud of creating a working feedback agent - one that could essentially verify the correctness of the code generated by the fool former agent, and have it regenerate code until it gets it right.</p>", "<p>We learned a lot about prompt engineering like few-shot learning, role, and effective use of context windows.</p>", "<p>Due to the fast pace of the hackathon setting, our team was forced to push off some features in order to complete the main functionality as fast as possible. One feature we wish we could have implemented is automatic documentation generation for newly created smart home commands, allowing our gemini model to instantly expand its context and capabilities according to the user's commands.</p>"]},
{"title": "Agent Acumen", "inspiration": ["<p>The inspiration for Agent Acumen struck during a conversation with my aunt, an orthodontist, who shared her frustrations about how much of her workday is consumed by repetitive, low-level tasks. These tasks divert her attention from patient care, which is her primary role and passion. Realizing that language models like ChatGPT can manage intellectual tasks, it sparked the idea to extend this capability into physical tasks through a robotic system. This would allow healthcare professionals to focus more on patient interactions rather than mundane tasks.</p>", "<p>Agent Acumen is designed to revolutionize the healthcare workspace by automating routine physical tasks through a highly intelligent robotic system. Utilizing advanced computer vision and machine learning, it precisely identifies, picks, and moves medical tools and supplies from one location to another as instructed. This automation includes tasks like moving tools from a disinfectant bath to a discard bin, effectively reducing the workload and minimizing the time spent on non-clinical tasks by healthcare staff.</p>", "<p>Building Agent Acumen within a tight 36-hour timeframe required rapid innovation and efficient problem-solving. Here’s how we brought it to life:</p>", "<p>Through these technologies, Agent Acumen brings the concept of low-level task automation from digital to physical, mirroring the efficiency seen in intelligent language models but in a tangible, impactful manner in healthcare settings.</p>", "<p>Building Agent Acumen was an ambitious project that involved complex integrations and ideas. Here are some of the key challenges we encountered:</p>", "<p>These challenges pushed our team to problem-solve at every turn.</p>", "<p>We're incredibly proud of several key achievements from this weekend's hackathon:</p>", "<p>The hackathon was not only a test of our technical skills but also a fantastic learning opportunity:</p>"], "what_it_does": ["<p>Agent Acumen is designed to revolutionize the healthcare workspace by automating routine physical tasks through a highly intelligent robotic system. Utilizing advanced computer vision and machine learning, it precisely identifies, picks, and moves medical tools and supplies from one location to another as instructed. This automation includes tasks like moving tools from a disinfectant bath to a discard bin, effectively reducing the workload and minimizing the time spent on non-clinical tasks by healthcare staff.</p>", "<p>Building Agent Acumen within a tight 36-hour timeframe required rapid innovation and efficient problem-solving. Here’s how we brought it to life:</p>", "<p>Through these technologies, Agent Acumen brings the concept of low-level task automation from digital to physical, mirroring the efficiency seen in intelligent language models but in a tangible, impactful manner in healthcare settings.</p>", "<p>Building Agent Acumen was an ambitious project that involved complex integrations and ideas. Here are some of the key challenges we encountered:</p>", "<p>These challenges pushed our team to problem-solve at every turn.</p>", "<p>We're incredibly proud of several key achievements from this weekend's hackathon:</p>", "<p>The hackathon was not only a test of our technical skills but also a fantastic learning opportunity:</p>"]},
{"title": "Amrit", "inspiration": ["<p>Over 8 million people a year around the world lose their lives to addiction-related issues, highlighting a global crisis that transcends geographical and cultural boundaries. In the midst of this challenge, our team at LA Hacks presents \"Amrit,\" a transformative solution designed to empower individuals battling addictions and stress-related disorders like alcoholism, nicotine dependence, depression, and anxiety.</p>", "<p>Amrit, which means 'elixir' in Sanskrit, symbolizes a source of renewal and hope. Our web-based application leverages advanced AI technology to create a personalized and engaging recovery journey. By selecting their specific struggle, users are guided through a series of tailored tasks aimed at promoting healthy habits and mental wellness. Success in these tasks is visually verified through our innovative use of Gemini AI’s vision capabilities, ensuring users remain accountable and motivated.</p>", "<p><em>Amrit</em> is a comprehensive web app that provides a personalized journey for users battling various addictions. It employs task-based challenges tailored to each user's specific addiction, fostering a sense of accomplishment and accountability. Users can track their progress, earn rewards, and access an AI-powered virtual therapist for emotional support.</p>", "<p>We developed <em>Amrit</em> by integrating multiple cutting-edge AI technologies, including Fetch.ai's agents, Gemini APIs for task generation and image recognition, and Delta V for the virtual therapist. The app dynamically generates tasks, verifies user submissions through image analysis, and provides a compassionate ear through the AI therapist.</p>", "<p>One of the significant challenges we encountered was integrating and orchestrating Fetch.ai agents to function seamlessly in a continuous loop, which was crucial for automating the task verification process in Amrit. Additionally, hosting the entire application on Heroku presented its own set of difficulties, particularly in ensuring stable deployment and efficient scaling to handle the computational demands of the AI components and user traffic.</p>", "<p>We are immensely proud of our ability to chain Fetch.ai agents together and successfully integrate this functionality into Amrit, creating a robust, automated system for task verification. Additionally, our team rapidly adapted to and effectively utilized a new tech stack, including Fetch.ai and Gemini API, which was a significant learning curve for us. Most importantly, we developed a fully functional end-to-end application that addresses real-world challenges, demonstrating our capability to transform an idea into a working solution within the constraints of a hackathon.</p>", "<p>Throughout the development of Amrit, our team gained invaluable insights into the complexities of AI-driven application development. We learned how to effectively chain and manage Fetch.ai agents, understanding the intricacies of AI interactions within an app. Delving into the Gemini API enhanced our skills in handling AI-powered image processing. Additionally, deploying on Heroku taught us about cloud application architectures and the challenges of scalable, stable application deployment. This project not only sharpened our technical skills but also deepened our appreciation for the potential of AI to address significant societal challenges like addiction recovery.</p>", "<p>The future of Amrit is full of exciting possibilities aimed at expanding its impact and user engagement. We plan to introduce a dedicated therapist on the platform to provide professional guidance and support, enhancing the AI therapist's capabilities. Additionally, creating community groups within Amrit will allow users to connect, share experiences, and support each other, fostering a strong community of recovery. We also aim to integrate social media functionalities, enabling users to share their achievements and milestones, which can help in normalizing recovery conversations and celebrating personal progress. Lastly, we are exploring partnerships with organizations to incorporate Amrit into wellness benefit programs for employees, thus broadening our reach and supporting more individuals in their journey to wellness.</p>"], "what_it_does": ["<p><em>Amrit</em> is a comprehensive web app that provides a personalized journey for users battling various addictions. It employs task-based challenges tailored to each user's specific addiction, fostering a sense of accomplishment and accountability. Users can track their progress, earn rewards, and access an AI-powered virtual therapist for emotional support.</p>", "<p>We developed <em>Amrit</em> by integrating multiple cutting-edge AI technologies, including Fetch.ai's agents, Gemini APIs for task generation and image recognition, and Delta V for the virtual therapist. The app dynamically generates tasks, verifies user submissions through image analysis, and provides a compassionate ear through the AI therapist.</p>", "<p>One of the significant challenges we encountered was integrating and orchestrating Fetch.ai agents to function seamlessly in a continuous loop, which was crucial for automating the task verification process in Amrit. Additionally, hosting the entire application on Heroku presented its own set of difficulties, particularly in ensuring stable deployment and efficient scaling to handle the computational demands of the AI components and user traffic.</p>", "<p>We are immensely proud of our ability to chain Fetch.ai agents together and successfully integrate this functionality into Amrit, creating a robust, automated system for task verification. Additionally, our team rapidly adapted to and effectively utilized a new tech stack, including Fetch.ai and Gemini API, which was a significant learning curve for us. Most importantly, we developed a fully functional end-to-end application that addresses real-world challenges, demonstrating our capability to transform an idea into a working solution within the constraints of a hackathon.</p>", "<p>Throughout the development of Amrit, our team gained invaluable insights into the complexities of AI-driven application development. We learned how to effectively chain and manage Fetch.ai agents, understanding the intricacies of AI interactions within an app. Delving into the Gemini API enhanced our skills in handling AI-powered image processing. Additionally, deploying on Heroku taught us about cloud application architectures and the challenges of scalable, stable application deployment. This project not only sharpened our technical skills but also deepened our appreciation for the potential of AI to address significant societal challenges like addiction recovery.</p>", "<p>The future of Amrit is full of exciting possibilities aimed at expanding its impact and user engagement. We plan to introduce a dedicated therapist on the platform to provide professional guidance and support, enhancing the AI therapist's capabilities. Additionally, creating community groups within Amrit will allow users to connect, share experiences, and support each other, fostering a strong community of recovery. We also aim to integrate social media functionalities, enabling users to share their achievements and milestones, which can help in normalizing recovery conversations and celebrating personal progress. Lastly, we are exploring partnerships with organizations to incorporate Amrit into wellness benefit programs for employees, thus broadening our reach and supporting more individuals in their journey to wellness.</p>"]},
{"title": "venture - new place, no problem", "inspiration": ["<p>We are adventure without the ads. It's common for people's lives to be influenced by ads. We wanted to create something that lets you influence your life, and no longer be controlled by mesmerizing social media algorithms. All of us have spent many days rotting away falling victim to laziness. Think about how many people have been in a city for a short amount of time (whether it is the extended business trip, family road trip, or extra long layover), yet did not go out and explore. Either the social media addiction took over,  they had no idea how many hidden gems were nearby, or they didn't want to plan. We wanted to make something that makes it easy for people to step outside of the known and into the outdoors.</p>", "<p>With Venture, you will say goodbye to missed opportunities and hello to new memories because Venture does it all for you.  Venture creates personalized itineraries for trips one day or less. People can specify locations, dates, start times, and end times and have a detailed itinerary in a matter of seconds. Venture takes the user's personal interests upon onboarding and applies that to create a custom itinerary that includes estimated costs, time, addresses, and images. Now, if you find yourself in a new place, you don't need to worry about the plan. New Place, No Problem.</p>", "<p>We built this web app using Svelte frontend and Flask backend. In order to get detailed personalized itineraries, we had multiple Fetch AI agents working with the Gemini API's. One agent called the API and then passed the information to another agent who figured out the schedule. From there we had 2 more Fetch AI agents that got information from Uber (Uber API) and generated images to match the generated itinerary.</p>", "<p>One of the biggest challenges we ran into was understanding how to have all the Fetch AI agents work together in order to generate the necessary information. Another challenge we ran into was integrating the backend into the frontend </p>", "<p>We are proud of implementing multiple agents and having a smooth itinerary generated.</p>", "<p>We learned about implementations for Fetch AI, how to work together efficiently, and learned about how Svelte worked with endpoints!</p>", "<p>In the future, we want to expand the capabilities of the generated itinerary to include receipts and confirmations in the same place. We also want to add a review option, where people can review what they liked and didn't like about the trip. This can then be used to learn their preferences for future trips. From a business perspective, we want to add possible partnerships with restaurants and activities in popular cities.</p>"], "what_it_does": ["<p>With Venture, you will say goodbye to missed opportunities and hello to new memories because Venture does it all for you.  Venture creates personalized itineraries for trips one day or less. People can specify locations, dates, start times, and end times and have a detailed itinerary in a matter of seconds. Venture takes the user's personal interests upon onboarding and applies that to create a custom itinerary that includes estimated costs, time, addresses, and images. Now, if you find yourself in a new place, you don't need to worry about the plan. New Place, No Problem.</p>", "<p>We built this web app using Svelte frontend and Flask backend. In order to get detailed personalized itineraries, we had multiple Fetch AI agents working with the Gemini API's. One agent called the API and then passed the information to another agent who figured out the schedule. From there we had 2 more Fetch AI agents that got information from Uber (Uber API) and generated images to match the generated itinerary.</p>", "<p>One of the biggest challenges we ran into was understanding how to have all the Fetch AI agents work together in order to generate the necessary information. Another challenge we ran into was integrating the backend into the frontend </p>", "<p>We are proud of implementing multiple agents and having a smooth itinerary generated.</p>", "<p>We learned about implementations for Fetch AI, how to work together efficiently, and learned about how Svelte worked with endpoints!</p>", "<p>In the future, we want to expand the capabilities of the generated itinerary to include receipts and confirmations in the same place. We also want to add a review option, where people can review what they liked and didn't like about the trip. This can then be used to learn their preferences for future trips. From a business perspective, we want to add possible partnerships with restaurants and activities in popular cities.</p>"]}
]